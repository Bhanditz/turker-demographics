\documentclass[11pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage[numbers]{natbib}
\usepackage[fit]{truncate}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{multirow}
\usepackage{dblfloatfix}
\usepackage{float}
\usepackage{subfigure}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox


\title{A large scale study of the languages spoken (by bilinguals) on Mechanical Turk}
\title{The Language Demographics of  Amazon Mechanical Turk}

\author{Ellie Pavlick \ \ \ \ \ \ \ \ \ \ Dmitry Kachaev \ \ \ \ \ \ \ \ \ \  Chris Callison-Burch \\
Computer and Information Science Department, University of Pennsylvania \\
Human Language Technology Center of Excellence, Johns Hopkins University \\
  }

\date{}

\begin{document}
\maketitle


\begin{abstract}
We present a large scale study of the languages spoken by bilingual workers on Mechanical Turk (MTurk).  
We establish a  methodology for determining the language skills of anonymous crowd workers that is more robust than simple surveying.  We validate workers' self-reported language skill claims by measuring their ability to correctly translate words, and by geolocating workers to see if they reside in countries where the languages are likely to be spoken. Rather than posting a one-off survey, we posted paid tasks consisting of 1,000 assignments to translate a total of 10,000 words in each of 100 languages.  Our study ran for weeks, and was highly visible on the MTurk crowdsourcing platform, increasing the chances that bilinguals workers would have seen it.  Our study was useful both to create bilingual dictionaries and to act as census of the bilingual speakers on MTurk.  We use this data to recommend languages with the largest speaker populations as good candidates for other researchers who want to  develop crowdsourced, multilingual technologies.


\end{abstract}

\section{Overview}
Crowdsourcing environments have become a key source of data for natural language processing research. Access to a fast, cheap, and flexible workforce has changed the way we collect data, and holds a great deal of promise for the future development of language technologies. Crowdsourced work has proven effective for collecting massive amounts of simple data annotations, such as annotating data for face recognition software and labeling sentiment in twitter data. As the demands and expectations of automated systems progress, and the complexity of the data required for training increases, however, it becomes natural to ask about the strengths and limitations of the crowd as annotators for natural language data.\\\\
We evaluate the language skills of bilingual workers on Amazon's Mechanical Turk, and their ability to provide translated data for statistical machine translation. Collecting parallel translated texts has traditionally been assumed to require a higher level of expertise than what is available from non-professional crowd workers. However, with an increasing number of active crowd workers located outside of the United States, the potential to access fluent speakers of lower resource languages makes crowdsourcing an attractive resource for translation.\\\\
We construct a task in which we aim to build bilingual dictionaries for over 100 languages, containing over 10,000 words each, using only translators available on Mechanical Turk. We collect demographic data relevant to the workers' language skills, and evaluate the quality of the translations submitted from workers across varying backgrounds and locations. Based on the data collected, we identify the strengths of Mechanical Turk as a source of multilingual data, and discuss best practices for ensuring high data quality from non-professional translators.

%%%%%%%%%%%%%%% NAT LANG PIE %%%%%%%%%%%
\begin{figure*}[h]
\centering
\includegraphics[width=7in]{figures/natlang-pie}
\caption{Self-reported native language of 2651 bilingual turkers}
\label{lang-pie}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Mechanical Turk}
Amazon's Mechanical Turk (MTurk) is an online crowdsourcing marketplace which gives employers and researchers access to a large, low-cost, human workforce. MTurk advertises `artificial artificial intelligence'; it provides a simple interface for accessing human skill, allowing small tasks, which are too difficult for computers, to be completed with the speed and scale of an automated process. When Amazon first introduced MTurk, payment was offered only in Amazon credits. As the service grew, payment switched to US dollars, and more recently has expanded to include foreign currencies, such as the rupee. Currently, MTurk claims over half a million workers from 190 countries, making it a compelling source of data for diverse natural language applications.\\\\
Within MTurk, those who need work completed (`requesters') post individual `Human Intelligence Tasks,' or just `HITs'. A typical HIT  requires a few minutes of work and pays a few cents, although there is significant variation across HITs. Once a HIT is posted, MTurk workers (`Turkers') are free to choose to complete the HITs which interest them. Worker identities remain anonymous to requesters, and all payment occurs through Amazon. Requesters are able to accept submitted work or reject work that they do not feel meets their standards, and only pay workers from whom they accepted tasks. \\\\
The convenience and low cost of using MTurk has made it popular among researchers. Because of the lack of oversight and the high potential for spam, however, most of the data collected from MTurk has been limited to simple tasks for which inter-annotator agreement is expected to be high (see Related Work section below). We explore the idea that MTurk's machinery is flexible enough to support more complex tasks requiring more nuanced quality control mechanisms, which could allow it to be a competitive alternative to professional data annotations.

\section{Related Work}
Crowdsourced data has been used in a variety of machine learning applications, including computer vision (Sorokin et al.), paraphrasing (Nakov et al.) and sentiment analysis (Snow et al.). More recently, in 2010, NAACL hosted a workshop on Creating Speech and Language Data with MTurk, in which MTurk was used to generate data for 24 different natural language tasks (CCB).\\\\
Typically, researchers rely on large amounts of MTurk data in order to compensate for the high likelihood of low quality labels. Snow et al. describes the success of using redundant non-expert labels to substitute for professional annotations, achieving comparable quality for much lower cost. The tasks from which these conclusions were drawn, however, were kept simple and easily-verifiable, with annotations restricted to either multiple choice or bounded numeric inputs. As NLP research advances, the level of expertise required from annotators advances as well. Callison-Burch et al. report success using MTurk to build parallel corpora for Machine Translation, a task which requires Turkers to speak two languages with a high level of proficiency.\\\\
As the use of MTurk has grown, researchers have become interested in who exactly makes up the Turker population. Early demographic studies by Ipeirotis revealed that Turkers are typically younger and more educated than the population as a whole. The study found that while most Turkers cite money as a motivation for working on MTurk, few cite it as their only motivation. A follow-up study by Ross et al. found that the majority of Turkers were located in the US, 30\% of Turkers were located in India, and that Indian Turkers tend to have lower incomes than US-based Turkers. Ross et al. also suggested that the international presence on MTurk has been growing over time. While there has not yet been a thorough investigation of Turkers' language abilities, Munro compiled survey responses of 2,000 Turkers, revealing that four of the six most represented languages come from India (the top six being Hindi, Malayalam, Tamil, Spanish, French, and Telugu). No study has yet been conducted to comprehensively assess the language skills of the growing number of international and bilingual Turkers or to analyze the potential of MTurk to support work in low-resource languages. \\\\

%%%%%%%%%%%%%%% HIT UIs %%%%%%%%%%%
\begin{figure}[h]
\centering
\includegraphics[width=3in]{figures/vocabulary_hit_mturk}
\caption{Translation HIT UI}
\label{tranhit}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=3in]{figures/synonyms_hit_mturk}
\caption{Evaluation HIT UI}
\label{synhit}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% SURVEY TABLE %%%%%%%%%%%
\begin{table*}[h]
\centering
\begin{tabular}{lc}\hline\hline\\
Is [HIT language/English] your native language? &129,049\\
How many years have you spoken [HIT language]?&159,807\\
How many years have you spoken English?&159,808\\
What country do you live in?&329,033\\\\
Current location (collected automatically)& 329,033\\\\
\hline\\
Total assignments& 329,033\\
Total unique workers& 6,034\\\\
\hline\hline
\end{tabular}
\caption{Demographic survey questions and number of assignments with valid responses for each}
\label{survey-tab}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Task Design}
The central task in this study was to use Mechanical Turk to create low cost, high quality bilingual dictionaries for over 100 languages in order to test the reliability and breadth of the knowledge of Mechanical Turk's bilingual population. \\\\
We chose the languages which were highly used on Wikipedia, in terms of the number of available articles. We compiled a list of all languages for which there existed at least 10,000 Wikipedia articles; we also included a small number of low resource languages for which there were approximately 1,000 available articles, giving a total of 119 languages to use in our task. For each of the languages, we chose the 1,000 most popular articles, and from these selected the 10,000 most frequent words. The resulting vocabularies served as the source side of our dictionaries.\\\\
The dictionary creation task was broken into two steps: a translation HIT and an evaluation HIT. Examples of the interface for each are shown in figures \ref{tranhit} and \ref{synhit}.  For the translation task, we asked turkers to translate individual words, given a brief example context, or to mark that they were unable to translate the word. Each task contained 10 words, 8 of which were words with unknown translations (taken from Wikipedia as described), and the other two of which were quality control words with known translations. The task paid one million dollars for the translation of 10 words. \\\\
For the evaluation task, Turkers were shown a pair of words, one which was a Turker's translation of one of the embedded quality control words, and the other which was a known gold-standard translation of the same word. Evaluators were asked whether the two words were synonyms, and chose between three answers: `Yes', `No', or `Related but not synonyms'. The last category was intended for word pairs such as `clouds' and `sky', which may be confusable to a non-native speaker but are not acceptable as dictionary translations. Turkers in the evaluation task could also flag the translated word as misspelled. Each HIT included 10 word pairs to be compared, and paid next to nothing. We describe the evaluation process further in the Measuring Quality section of this paper.\\\\
Quality control for the evalutation HIT consisted of embedded word pairs which were either known to be synonyms or were known to be unrelated. Whether the evaluators correctly labeled the known word pairs could be assessed automatically, allowing the evaluation HIT to be the final step of the dictionary creation pipeline.\\\\

\section{Turker Demographics}
At the start of each HIT, Turkers were asked to complete a brief survey about their language abilities. Valid responses to all survey questions were not required in order to complete the HIT; survey questions and the number of valid responses received for each are listed in table \ref{survey-tab}. Although it was not required, Turkers who completed multiple HIT assignments could fill out the survey multiple times. This enabled some Turkers to report multiple native languages (see figure \ref{numlangs-tab}). While most results presented are calculated across all Turkers, figures given for distributions across native languages are calculated only from Turkers who reported a single native language.\\

%%%%%%%%%%%%%%% NUM LANGUAGES TABLE %%%%%%%%%%%
\begin{figure}[h]
\begin{tabular}{ccc}\hline\hline
IGNORE TABLE& WILL FIX SOON&\\
&\# languages&\# Turkers\\
\hline
No languages&0&2555\\\\
One language&1&2651\\\\
Multiple languages&2&684\\
&3&94\\
&4&23\\
&5&7\\
&6&8\\
&7&4\\
&8&1\\
&9&2\\
&10&3\\
&15&1\\
\hline\hline
\end{tabular}
\label{numlangs-tab}\\
\caption{Number of native languages reported during demographic survey.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% ASSIGNMENT SCATTER %%%%%%%%%%%
\begin{figure*}[h]
\centering
\includegraphics[width=6in]{figures/assign-turk-scatter}
\caption{Number of assignments and number of turkers for each language. Each dot represents a language.}
\label{ass-scatter}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure \ref{ass-scatter} shows the volume of HITs completed and the number of workers participating for each language. While the bulk (22\%) of bilingual turkers report English as their native language (see figure \ref{lang-pie}), Mechanical Turk's capacity to support the Indian languages is apparent. Hindi, Tamil, and Malayalam are especially well represented in terms of number of active translators, and Urdu, Telugu, and Macedonian are particularly productive in terms of number of assignments completed. As shown in figure \ref{langgeo-bar}, 11 of the top 25 languages, in terms of number of assignments submitted, were Indian languages, and the assignments for 15 of the top 25 were completed mostly or entirely by turkers located in India. 

%%%%%%%%%%%%%%% LOCATION BAR %%%%%%%%%%%
\begin{figure*}[h]
\centering
\includegraphics[width=6in]{figures/assign-langgeo-sorted}
\caption{Geolocation of turkers speaking 40 most represented languages, in terms of number of assignments submitted}
\label{langgeo-bar}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Measuring Data Quality}
The primary challenge of using crowdsourced language data is the widely variable quality of the collected annotations. While it is possible to strengthen the signal by collecting redundant labels, this strategy becomes difficult in practice for more complex data annotation. In the case of translation, even professional translations can be expected to vary significantly, and fully automatic comparison of free form text is itself an open research question.\\\\ 
In order to address the quality of the translations we received on the dictionary task, we constructed a pipeline in which the output of the translation HIT was reviewed by humans in a second evaluation HIT, described above. We quantified the quality of each translation assignment based on the output of the evaluation HIT. The quality of an assignment was scored as the fraction of known words which were acceptably translated: i.e, the supplied translation was judged to be synonymous with the gold standard translation. Since each assignment had either one or two known words embedded, each assignment was assigned a score of either 0, 0.5, or 1.\\\\

%%%%%%%%%%%%%%% MISREPORT TABLE %%%%%%%%%%%
\begin{figure}[h]
\centering
\begin{tabular}{cccc}\hline\hline\\
&Avg. & 99\%&\\
&Quality & Conf. Int.&n\\
Misreport&0.252&(0.244, 0.260)&10,479\\
Correct&0.282&(0.280, 0.283)&296,911\\
Overall&0.281&(0.279, 0.282)&307,390\\\\
\hline\hline
\end{tabular}
\label{mism-tab}
\caption{Quality of translations recieved from truthful versus misreporting Turkers.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% HITLANG QUALITY BAR %%%%%%%%%%%
\begin{figure*}[h]
\centering
\includegraphics[width=6in]{figures/quality-hitlang}
\caption{Translation quality of by source language}
\label{hitlangqual-bar}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% MISREPORT TABLE %%%%%%%%%%%
\begin{figure}[h]
\centering
\begin{tabular}{cccc}
Language&Total&Quality $\geq$ 0.5& Quality $>$ 0.5\\\hline\hline
af&9983.0&7199.0&1679.0\\
am&8341.0&6646.0&2489.0\\
an&3080.0&938.0&228.0\\
ar&9994.0&9638.0&5024.0\\
ast&9968.0&9035.0&3808.0\\
az&9975.0&7034.0&1983.0\\
bcl&9940.0&4718.0&384.0\\
be&9964.0&7008.0&2039.0\\
bg&9982.0&8389.0&2622.0\\
bn&9984.0&8915.0&3761.0\\
bo&9920.0&1744.0&64.0\\
bpy&9854.0&6439.0&2604.0\\
br&9978.0&6188.0&1722.0\\
bs&9860.0&7110.0&1913.0\\
ca&9993.0&7318.0&1533.0\\
ceb&9445.0&5394.0&1202.0\\
cs&9957.0&6844.0&1342.0\\
cy&9971.0&5369.0&942.0\\
da&9983.0&8827.0&2843.0\\
de&9921.0&8594.0&2609.0\\
diq&33.0&8.0&0\\
el&9994.0&8150.0&2630.0\\
eo&9962.0&7261.0&1574.0\\
es&9835.0&8127.0&2265.0\\
eu&9993.0&9031.0&3365.0\\
fa&5565.0&3527.0&1070.0\\
fi&9968.0&8527.0&2499.0\\
fr&9900.0&8472.0&2859.0\\
fy&9962.0&6840.0&1618.0\\
ga&9914.0&8421.0&2711.0\\
gl&9987.0&7843.0&2606.0\\
gu&9982.0&9916.0&9294.0\\
he&8271.0&5807.0&1854.0\\
hi&9949.0&9519.0&6066.0\\
hr&9949.0&8723.0&2589.0\\
ht&9938.0&8874.0&3323.0\\
hu&9885.0&8304.0&2246.0\\
hy&1758.0&1314.0&613.0\\
id&9770.0&7090.0&1363.0\\
ilo&9705.0&7510.0&1303.0\\
io&147.0&58.0&41.0\\
is&9913.0&6163.0&1907.0\\
it&9755.0&8270.0&2312.0\\
ja&8053.0&4204.0&1501.0\\
jv&9883.0&7126.0&1404.0\\
ka&5557.0&3837.0&1437.0\\
kk&40.0&8.0&8.0\\
kn&9993.0&9928.0&8591.0\\
\end{tabular}
\label{mism-tab}
\end{figure}
\clearpage
\begin{figure}[h]
\centering
\begin{tabular}{cccc}
ko&7159.0&3244.0&1094.0\\
ku&149.0&70.0&0\\
lb&9965.0&6786.0&1498.0\\
lt&9975.0&8044.0&2197.0\\
lv&9981.0&7343.0&2008.0\\
mg&201.0&46.0&5.0\\
mk&9845.0&9373.0&5138.0\\
ml&9991.0&9795.0&7047.0\\
mr&9993.0&9840.0&7540.0\\
ms&9766.0&8348.0&2810.0\\
nap&6540.0&2171.0&462.0\\
nds&7324.0&3179.0&615.0\\
ne&9972.0&9570.0&6100.0\\
new&9976.0&6530.0&1018.0\\
nl&9932.0&8924.0&3106.0\\
nn&9994.0&8355.0&2529.0\\
no&9941.0&8965.0&3070.0\\
pa&9881.0&9464.0&6611.0\\
pam&9909.0&6960.0&1384.0\\
pl&9813.0&7207.0&1367.0\\
pms&9285.0&4104.0&688.0\\
ps&9727.0&5309.0&794.0\\
pt&9857.0&7638.0&1554.0\\
qu&67.0&0&0\\
ro&9875.0&8849.0&2804.0\\
ru&9977.0&0&0\\
scn&9983.0&8446.0&3476.0\\
sd&9357.0&3866.0&139.0\\
sh&9930.0&8120.0&2043.0\\
sk&9977.0&6039.0&1435.0\\
sl&9889.0&5871.0&1092.0\\
so&9908.0&2925.0&263.0\\
sq&9924.0&8237.0&2468.0\\
sr&9890.0&9005.0&3428.0\\
su&9947.0&5818.0&891.0\\
sv&9856.0&8127.0&2241.0\\
sw&9770.0&8148.0&2186.0\\
ta&9979.0&9687.0&6948.0\\
te&9933.0&9085.0&5265.0\\
th&4252.0&2381.0&883.0\\
tl&9280.0&7173.0&2475.0\\
tr&9941.0&8011.0&1896.0\\
tt&40.0&8.0&8.0\\
uk&9787.0&6349.0&2208.0\\
ur&9864.0&3076.0&1033.0\\
uz&9874.0&6643.0&1122.0\\
vi&8276.0&7027.0&2935.0\\
wa&49.0&28.0&11.0\\
war&8579.0&5273.0&874.0\\
wo&9759.0&5691.0&0\\
yo&2367.0&2119.0&1245.0\\
zh&3647.0&2812.0&1037.0\\
\hline\hline
\end{tabular}
\label{mism-tab}
\caption{Quality of translations recieved from truthful versus misreporting Turkers.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Measured in this way, the average quality score across all HITs was just under 0.3. In general, countries which produced more translations did not produce lower quality translations, with India, Macedonia, and the US falling very close to average quality (figure \ref{quality-scatter}). In fact, likely as a result of the large number of India-based Turkers, most of the Indian languages fall above average quality, including Hindi, Telugu, Malayalam, Marathis, Tamil, Gujarati, Kannada, Bengali, and Punjabi, shown in figure \ref{hitlangqual-bar}. The notable exception is Urdu, which produced below average translations, likely due to the relatively low number of unique translators, making quality more susceptible to individual careless workers (see figure \ref{ass-scatter}). \\\\

%%%%%%%%%%%%%%% NATIVE LANGUAGE QUALITY BAR %%%%%%%%%%%
\begin{figure*}[h]
\centering
\includegraphics[width=6in]{figures/quality-natlang-sorted}
\caption{Translation quality of native and non-native speakers}
\label{natlangqual-bar}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Interestingly, native speakers do not consistently outperform non-native speakers (see figure \ref{natlangqual-bar}). In certain unique cases, such as Vietnam, non-native speakers produce significantly better translations than native speakers, possibly due to a large population of fluent US-based workers (figure \ref{langgeo-bar}). In general, the quality difference between native and non-native speakers is not significant.	\\\\

%%%%%%%%%%%%%%% QUALITY / NUM ASSIGN SCATTER %%%%%%%%%%%
\begin{figure*}[h]
\centering
\includegraphics[width=6in]{figures/quality-scatter-avgturkers-country-labeled}
\caption{Quality of translations by country. Each circle is a country, sized proportional to the number of active Turkers from that country.}
\label{quality-scatter}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Self-reported country information is typically reliable. Three quarters of Turkers who reported their native language reported a single native language consistantly across all assignments (table \ref{numlangs-tab}), and of those reporting multiple native languages, the majority listed English in addition to the HIT's source language, meaning less than 5\% of Turkers gave unreasonable responses. Across all assignments, 96\% of reported locations agreed with automatically reported locations, although assignments in which Turkers misreported their location averaged 10\% lower in quality than those in which the reported location was accurate, shown in table \ref{mism-tab}.\\\\

\section{Discussion}
MTurk has a strong and diverse presence of bilingual workers, making it a promising resource for researchers and developers of multilingual systems. Although unfiltered data can contain large amounts of noise, crowdsourced pipelines, which contain human oversight as a means of evaluation, offer a feasible way of ensuring high quality data, even on tasks which require more complex labels. While MTurk does offer the ability to restrict workers based on country, embedded per-task controls which are checked either automatically when possible, or manually in a second-pass HIT, are likely to provide higher quality data than naive demographic filters. \\\\
Also, with intelligent quality control techniques, MTurk can prove that $P = NP$, solve the problems in the Middle East, and allow you to eat carbs and still lose weight.

%\begin{thebibliography}{}

%\bibitem[\protect\citename{}yrs]{name}
%name
%\newblock year
%\newblock title
%\newblock In {\em journal}.

%\end{thebibliography}

\end{document}
