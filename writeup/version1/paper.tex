\documentclass[11pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage[fit]{truncate}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{multirow}
\usepackage{dblfloatfix}
\usepackage{float}
\usepackage{subfloat}
\usepackage{subcaption}

\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{5cm}    % Expanding the titlebox

\newcommand{\affliationPenn}{\ensuremath{{}^\text{1}}}
\newcommand{\affliationJHU}{\ensuremath{{}^\text{2}}}

\title{A large scale study of the languages spoken (by bilinguals) on Mechanical Turk}
\title{The Language Demographics of  Amazon Mechanical Turk}

\author{Ellie Pavlick\affliationPenn \ \ \ \ \ Ann Irvine\affliationJHU  \ \ \ \ \ Dmitry Kachaev\affliationJHU  \ \ \ \ \  Chris Callison-Burch\affliationPenn$^{,}$\affliationJHU \\
\affliationPenn Computer and Information Science Department, University of Pennsylvania \\
\affliationJHU Human Language Technology Center of Excellence, Johns Hopkins University \\
  }
  
% Anonymized for submission
\author{}

\date{}

\begin{document}
\maketitle

\begin{abstract}
We present a large scale study of the languages spoken by bilingual workers on Mechanical Turk (MTurk).  
We establish a  methodology for determining the language skills of anonymous crowd workers that is more robust than simple surveying.  We validate workers' self-reported language skill claims by measuring their ability to correctly translate words, and by geolocating workers to see if they reside in countries where the languages are likely to be spoken. Rather than posting a one-off survey, we posted paid tasks consisting of 1,000 assignments to translate a total of 10,000 words in each of 119 languages.  Our study ran for several weeks, and was highly visible on the MTurk crowdsourcing platform, increasing the chances that bilingual workers would complete it.  Our study was useful both to create bilingual dictionaries and to act as census of the bilingual speakers on MTurk.  We use this data to recommend languages with the largest speaker populations as good candidates for other researchers who want to  develop crowdsourced, multilingual technologies.

\end{abstract}

\section{Overview}
Crowdsourcing is a promising new mechanism for collecting data for natural language processing research. Access to a fast, cheap, and flexible workforce allows us to collect new types of data, potentially enabling new language technologies.
Because crowdsourcing platforms like Amazon Mechanical Turk (MTurk) give researchers access to a worldwide workforce, one obvious application of crowdsourcing is the creation of multilingual technologies. 
With an increasing number of active crowd workers located outside of the United States, there is even the potential to reach fluent speakers of lower resource languages.
In this paper, we investigate the feasibility of hiring language informants on MTurk by conducting the first large-scale demographic study of the languages spoken by workers on the platform. 

There are several complicating factors when trying to take a census of workers on MTurk.  The workers' identities are anonymized, and Amazon provides no information about their countries of origin or their language abilities.  Posting a simple survey to have workers report this information may be inadequate, since (a) many workers may never see the survey, (b) many opt not to do one-off surveys since potential payment is low, and (c) validating the answers of respondents is not straightforward. 

Our study establishes a methodology for determining the language demographics of anonymous crowd workers that is more robust than simple surveying. We ask workers what languages they speak and what country they live in, and validate their claims by measuring their ability to correctly translate words and by recording their geolocation.  To increase the visibility and the desirability of our tasks, we post 1,000 assignments in each of 119 languages.  These tasks each consist of translating 10 foreign words into English.  Two of the 10 words have known translations, allowing us to validate that the workers' translations are accurate.  We construct bilingual dictionaries with up to 10,000 entries, with the majority of entries being new. 

Surveying thousands of workers allows us to analyze current speaker populations for more than 100 languages.  The data also allows us to answer questions like: 
How quickly is work completed in a given language? 
Are crowdsourced translations reliably good? 
How often do workers misrepresent their language abilities to obtain financial rewards? 

%%%%%%%%%%%%%%% MAP OF WORKER LOCATIONS %%%%%%%%%%%
\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{../version1-figures/map.png}
\caption{The number of workers per country.  This map was generated based on geolocating the IP address of  5,277 workers in our study.  The size of the circles represents the number of workers from each country.  The two largest are India (2,017 workers) and the United States (904).  To calibrate the sizes: the Philippines has 147 workers, Egypt has 25, Russia has 10, and Sri Lanka has 4.}
\label{map}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background and Related Work}
Amazon's Mechanical Turk (MTurk) is an online marketplace for work that gives employers and researchers access to a large, low-cost, workforce. MTurk allows employers to provide micro-payments in return for workers completing micro-tasks.  The basic units of work on MTurk are called `Human Intelligence Tasks' (HITs).  MTurk was designed to accommodate tasks that are difficult for computers, but simple for people. This facilitates research into human computation, where people can be treated as a function call \cite{vonAhnThesis,Little2009,quinn-bederson:2011}.  It has application to research areas like human-computer interaction \cite{bigham-et-al:2010,bernstein-et-al:2010}, computer vision  \cite{sorkin-forsyth:2008,deng-et-al:2010,rashtchian:10}, speech processing \cite{marge:10,lane-EtAl:2010:MTURK,Parent-Eskenazi:2011,Eskenazi:2013:crowdsourcing-speech-book},  and natural language processing \cite{Snow2008,callisonburch-dredze:2010:MTURK,laws-scheible-schutze:2011:EMNLP}. 

\nocite{novotney-callisonburch:2010:NAACLHLT}

On MTurk, researchers who need work completed are called `Requesters', and workers are often referred to as `Turkers'.  MTurk is a true market, meaning that Turkers are free to choose to complete the HITs which interest them, and Requesters can price their tasks competitively to try to attract workers and have their tasks done quickly \cite{faridani-et-al:2011,singer-mittal:2011}. Turkers remain anonymous to Requesters, \nocite{Lease2013}
and all payment occurs through Amazon. Requesters are able to accept submitted work or reject work that does not meet their standards.  Turkers are only paid if a Requester accepts their work. 

Several reports examine Mechanical Turk as an economic market \cite{ipeirotis:2010:marketplace,lehdonvirta-ernkvist:2011}.  When Amazon introduced MTurk, it first offered payment only in Amazon credits, and later offered direct payment in US dollars. More recently, it has expanded to include one foreign currency, the Indian rupee. Despite its payments being limited to two currencies or Amazon credits, MTurk claims over half a million workers from 190 countries \cite{AmazonRequesterTour}.  This suggests that its worker population should represent a diverse set of languages.

A demographic study by \newcite{ipeirotis:2010:demographics} focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income.  The study contrasted Indian and US workers. \newcite{Ross2010} completed a longitudinal follow-on study. 
A number of other studies have informally investigated Turkers' language abilities.  \newcite{munro-tily:2011} compiled survey responses of 2,000 Turkers, revealing that four of the six most represented languages come from India (the top six being Hindi, Malayalam, Tamil, Spanish, French, and Telugu).  \newcite{irvine:10} had Turkers evaluate the accuracy of translations that had been automatically inducted from monolingual texts.  They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their workers (India, the US, Romania, Pakistan, Macedonia, Latvia, Bangladesh and the Philippines).  Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers' language skills for rare languages, which we address in this paper. 

Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume  translation on demand \cite{Germann2001}.  \newcite{ambati_act} conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole.  In a study of 2000 Urdu sentences, 
\newcite{zaidan-callisonburch:2011:ACL-HLT2011a} presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. 
\newcite{Zbib-etal:2012:NAACL} used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data.  \newcite{post-callisonburch-osborne:2012:WMT} used MTurk to build parallel corpora between English and six Indian languages, ranging from .5M to 1.5M words, and use them to demonstrate the efficacy of syntactic machine translation on verb-final languages.  
Several researchers have examined cost optimization using active learning techniques to select the most useful sentences or fragments to translate \cite{ambati_naacl,bloodgood-callisonburch:2010:ACL,AmbatiThesis}.

To contrast our research with previous work, the main contributions of this paper are: (1) a robust methodology for assessing the bilingual  skills of anonymous workers, (2) the largest-scale census to date of language skills of workers on MTurk, and (3) a detailed analysis of the data gathered in our study.

%%%%%%%%%%%%%%% NAT LANG PIE TABLE %%%%%%%%%%%
\begin{table}
\footnotesize
\begin{tabular}{lrlrlr}\hline\hline
%Language&\# Turkers\\
\hline
English & 689 & Tamil & 253 & Malayalam & 219 \\
Hindi & 149 & Spanish & 131 & Telugu & 87 \\
Chinese & 86 & Romanian & 85 & Portuguese & 82 \\
Arabic & 74 & Kannada & 72 & German & 66 \\
French & 63 & Polish & 61 & Urdu & 56 \\
Tagalog & 54 & Marathi & 48 & Russian & 44 \\
Italian & 43 & Bengali & 41 & Gujarati & 39 \\
Hebrew & 38 & Dutch & 37 & Turkish & 35 \\
Vietnamese & 34 & Macedonian & 31 & Cebuano & 29 \\
Swedish & 26 & Bulgarian & 25 & Swahili & 23 \\
Hungarian & 23 & Catalan & 22 & Thai & 22 \\
Lithuanian & 21 & Punjabi & 21 & Others & $\leq$ 20 \\
\hline\hline
\end{tabular}
\normalsize
\caption{Self-reported native language of 3,154 bilingual Turkers. Not shown are 54 languages with $\leq$20 speakers. 
We omit 1,797 Turkers who did not report their native language, 243 who reported 2 native languages, and 83 with $\geq$3 native languages.}\label{lang-pie}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Survey Design}
The central task in this study was to investigate Mechanical Turk's bilingual population.  We accomplished this through self-reported surveys combined with a HIT to translate individual words for over 100 languages.  We evaluate the accuracy of the workers' translations against known translations.  In cases where these were not exact matches, we used a second pass monolingual HIT, which asked English speakers to evaluate if a worker-provided translation was a synonym of the known translation.

\paragraph{Demographic questionnaire}

At the start of each HIT, Turkers were asked to complete a brief survey about their language abilities. The survey asked the following questions:
\begin{itemize}
\item Is [language] your native language? 
\item How many years have you spoken [language]? 
\item Is English your native language? 
\item How many years have you spoken English?
\item What country do you live in?
\end{itemize}
We automatically collected each worker's current location by geolocating their IP address.  A total of 5,277 unique workers completed our HITs.  Among those, 3,890 provided responses to our survey questions, and we were able to geolocate 5,050.  Figure \ref{map} plots the location of workers across 111 countries.  Table \ref{lang-pie} gives the most common self-reported native languages. 

\paragraph{Selection of languages}

We drew our data from the different language versions of Wikipedia.   We selected the 100 languages with the largest number of articles,\footnote{\url{http://meta.wikimedia.org/wiki/List_of_Wikipedias}} and then selected an additional 19 low resource languages, giving a total of 119 languages (Table \ref{wikipedia-buckets}). For each language, we chose the 1,000 most viewed articles over a 1 year period,\footnote{\url{http://dumps.wikimedia.org/other/pagecounts-raw/}} and extracted the 10,000 most frequent words from them. The resulting vocabularies served as the input to our translation HIT.

\begin{table}[h]
\scriptsize 
\begin{center}
\begin{tabular}{|p{0.95\linewidth}|}
\hline
{\sc 500k+ articles:} German (de), English (en), Spanish (es), French (fr), Italian (it), Japanese (ja), Dutch (nl), Polish (pl), Portuguese (pt), Russian (ru)\\
\hline
{\sc 100k-500k articles:} Arabic (ar), Bulgarian (bg), Catalan (ca), Czech (cs), Danish (da), Esperanto (eo), Basque (eu), Persian (fa), Finnish (fi), Hebrew (he), Hindi (hi), Croatian (hr), Hungarian (hu), Indonesian (id), Korean (ko), Lithuanian (lt), Malay (ms), Norwegian (Bokmal) (no), Romanian (ro), Slovak (sk), Slovenian (sl), Serbian (sr), Swedish (sv), Turkish (tr), Ukrainian (uk), Vietnamese (vi), Waray-Waray (war), Chinese (zh)\\
\hline
{\sc 10k-100k articles:}
Afrikaans (af) Amharic (am) Asturian (ast) Azerbaijani (az) Belarusian (be) Bengali (bn) Bishnupriya Manipuri (bpy) Breton (br) Bosnian (bs) Cebuano (ceb) Welsh (cy) Zazaki (diq) Greek (el) West Frisian (fy) Irish (ga) Galician (gl) Gujarati (gu) Haitian (ht) Armenian (hy) Icelandic (is) Javanese (jv) Georgian (ka) Kannada (kn) Kurdish (ku) Luxembourgish (lb) Latvian (lv) Malagasy (mg) Macedonian (mk) Malayalam (ml) Marathi (mr) Neapolitan (nap) Low Saxon (nds) Nepali (ne) Newar / Nepal Bhasa (new) Norwegian (Nynorsk) (nn) Piedmontese (pms) Sicilian (scn) Serbo-Croatian (sh) Albanian (sq) Sundanese (su) Swahili (sw) Tamil (ta) Telugu (te) Thai (th) Tagalog (tl) Urdu (ur) Yoruba (yo)\\ 
\hline
{\sc $<$10k articles:} Central Bicolano (bcl) Tibetan (bo) Ilokano (ilo) Punjabi (pa) Kapampangan (pam) Pashto (ps) Sindhi (sd) Somali (so) Uzbek (uz) Wolof (wo)\\

\hline
\end{tabular}
\end{center}
\normalsize
\caption{A list of the languages that were used in our study, grouped by the number of Wikipedia articles in the language.  Each language's code is given in parentheses.  These language codes are used in other figures throughout this paper.}\label{wikipedia-buckets}
\end{table}%


%%%%%%%%%%%%%%% HITLANG QUALITY BAR %%%%%%%%%%%
\begin{figure*}[h]
\begin{center}
\includegraphics[width=\linewidth]{../version1-figures/quality_bar.png}
\caption{Translation quality for languages with at least 50 Turkers.  The dark blue bars indicate the proportion of translations which exactly matched gold standard translations, and light blue indicate translations which were judged to be correct synonyms.}
\label{hitlangqual} 
\end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\paragraph{Translation HIT}

For the translation task, we asked Turkers to translate individual words.  We showed each word in the context of three sentences that were drawn from Wikipedia.  
Turkers were allowed to mark that they were unable to translate a word. Each task contained 10 words, 8 of which were words with unknown translations, and 2 of which were quality control words with known translations.   
We gave special instruction for translating names of people and places, giving examples of how to handle `Barack Obama' and `Australia' using their interlanguage links. For languages with non-Latin alphabets, names were transliterated. 

The task paid \$0.15 for the translation of 10 words.  
Each set of 10 words was independently translated by three separate workers.  5,277 workers completed 308,079 translations assignments, totaling more than 3 million words, over a period of three and a half months.

\paragraph{Gold standard translations} 
A set of gold standard translations were automatically harvested from Wikipedia  for every language to use as embedded controls. We used Wikipedia's inter-language links to pair titles of English articles with their corresponding foreign article's title.  To get a more translatable set of pairs, we excluded any pairs where: (1) the English word was not present in the WordNet ontology  \cite{miller1995wordnet}, (2) either article title was longer than a single word, (3) the English wikipedia page was a subcategory of person or place, or (4) the English and the foreign titles were identical or a substring of the other.

\paragraph{Manual evaluation of non-identical translations}
We counted all translations that exactly matched the gold standard translation as correct.  For non-exact matches we created a second-pass quality assurance HIT.  
Turkers were shown a pair of English words, one of which was a Turker's translation of the foreign word used for quality control, and the other of which was the gold-standard translation of the foreign word. Evaluators were asked whether the two words had the same meaning, and chose between three answers: `Yes', `No', or `Related but not synonymous.'  Examples of meaning equivalent pairs include: $<${\it petroglyphs, rock paintings}$>$, $<${\it demo, show}$>$ and  $<${\it loam, loam: soil rich in decaying matter}$>$.  Non-meaning equivalents included: $<${\it assorted, minutes}$>$, and $<${\it major,} URL of image$>$.  Related items were things like $<${\it sky, clouds}$>$. Misspellings like $<${\it lactation, lactiation} $>$ were judged to have same meaning, and were marked as misspelled.   Three separate Turkers judged each pair, allowing majority votes for difficult cases. 


We checked Turkers who were working on this task by embedding pairs of words which were either known to be synonyms (drawn from WordNet) or  unrelated (randomly chosen from a corpus). 
Automating approval/rejections for the second-pass evaluation allowed the whole pipeline to be run automatically.  Caching judgments meant that we ultimately needed only 
20,955 synonym tasks to judge all of the submitted translations (a total of 55,880 unique non-matching word pairs).  These were completed by an additional 1,006 workers.  Each of these assignments included 10 word pairs and paid \$0.10.



\section{Measuring Translation Quality}

We calculate the quality of translations on the level of individual assignments and aggregated over languages and countries.  We define an assignment's quality as the proportion of controls that are correct in a given assignment, where correct means exactly correct or judged to be synonymous or related.
\begin{align}	
	\text{Quality}(a_i) = \frac{1}{k_i}\sum\limits_{j=1}^{k_i}\mathbf{\delta}(tr_{ij} \in \texttt{syns[$g_j$]})
\end{align}	
where $a_i$ is the $i^{th}$ assignment, $k_i$ is the number of controls in $a_i$, $tr_{ij}$ is the Turker's provided translation of control word $j$ in assignment $i$, $g_j$ is the gold standard translation of control word $j$, \texttt{syns[$g_j$]} is the set of words judged to be synonymous with $g_j$ and includes $g_j$, and $\mathbf{\delta}(x)$ is Kronecker's delta and takes value 1 when $x$ is true. 
Most assignments had two known words embedded, so most assignments had scores of either 0, 0.5, or 1. 

%Since judging overall quality for a language with assignment quality scores is biased towards to small numbers of highly active Turkers, we instead report language quality scores as the average per-Turker quality, where a Turker's quality is  the average quality of all the assignments that she completed:
%\begin{align}	
%	\text{Quality}(t_i) = \frac{\sum_{a_j \in \texttt{assigns[$i$]}\text{Quality}(a_j)}}{\mid \texttt{assigns[$i$]} \mid}
%\end{align}	
%where $t_i$ is the $i^{th}$ Turker, \texttt{assigns[$i$]} is the assignments completed by Turker $i$, and Quality($a$) is as above.
%

Quality for a language is then given by
\begin{align}	
	\text{Quality}(l_i) = \frac{\sum_{a_j \in \texttt{assigns[$i$]}\text{Quality}(a_j)}}{\mid \texttt{assigns[$i$]} \mid}
\end{align}	
where $l_i$ is the ith language, \texttt{assigns[$i$]} is the set of completed assignments for language $i$, and Quality($a$) is as above. Quality for a country is computed the same way.

%Quality for a language is then given by
%\begin{align}	
%	\text{Quality}(l_i) = \frac{\sum_{t_j \in \texttt{Turkers[$i$]}\text{Quality}(t_j)}}{\mid \texttt{Turkers[$i$]} \mid}
%\end{align}	
%where $l_i$ is the ith language, \texttt{Turkers[$i$]} is the set of Turkers who completed assignments for language $i$, and Quality($t$) is as above.


Figure \ref{hitlangqual} shows the translation quality for for languages with contributions from at least 50 workers.  



\section{Data Analysis}

We performed an analysis of our data to address the following questions:
\begin{itemize}
\item How quickly can we expect work to be completed in a particular language? 
\item Do workers accurately represent their language abilities?  Should we constrain tasks by region? 
\item Is the use of machine translation prevalent?  
\item Are our gold standard translations valid? 
%\item What is the quality of crowdsourced translations without quality control versus with it? 
\end{itemize}


\paragraph{Speed of completion}


Figure \ref{completion-time} gives the completion times for 40 languages.  
The 10 languages to finish in the shortest amount of time were: Tamil, Malayalam, Telugu, Hindi, Macedonian, Spanish, Serbian, Romanian, Gujarati, and Marathi. Seven of the ten fastest languages are from India, which is unsurprising given the geographic distribution of workers.  Some languages follow the pattern of having a smattering of assignments completed early, with the rate picking up later. 

%%%%%%%%%%%%%%% COMPLETION TIME CHART %%%%%%%%%%%
\begin{figure}[h]
\includegraphics[height=\linewidth,angle=270]{../final-figures/completetime}
\caption{Days to complete the translation HITs for 40 of the languages. Tick marks represent the completion of individual assignments. }
\label{completion-time}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%% TURKER QUAL SUMMARY TABLE %%%%%%%%%%%
\begin{table*}
\scriptsize
\begin{center}
\begin{tabular}{lllll}
\hline\hline
&\multicolumn{2}{c}{Avg. Turker quality (\# Ts)}&Primary locations&Primary locations\\
&In region&Out of region&of Turkers in region&of Turkers out of region\\
\hline\hline
Hindi & 0.636 (291) & \textbf{0.715} (6) & India (279) Uae (5) Uk (3)  & Russia (1) Oman (1) Denmark (1) \\
Tamil & \textbf{0.652} (267) * & 0.250 (2) & India (260) Unitedstates (3) Canada (2)  & Tunisia (1) Egypt (1)  \\
Malayalam & 0.763 (233) & \textbf{0.827} (2) & India (222) Uae (6) Unitedstates (3)  & Saudiarabia (1) Maldives (1) \\
Spanish & \textbf{0.794} (152) & 0.788 (12) & Unitedstates (98) Mexico (16) Spain (12)  & India (10) Newzealand (1) Macedonia (1)  \\
French & 0.744 (146) & \textbf{0.816} (11) & India (50) Unitedstates (37) France (22)  & Greece (2) Netherlands (1) Japan (1) \\
Chinese & \textbf{0.597} (115) & 0.597 (19) & Unitedstates (74) Singapore (13) China (9)  & Hongkong (6) Australia (3) Germany (2)  \\
German & \textbf{0.818} (81) & 0.769 (37) & Germany (45) Unitedstates (22) Austria (6)  & India (31) Netherlands (1) Greece (1)  \\
Amharic & \textbf{0.136} (16) * & 0.009 (99) & Unitedstates (14) Ethiopia (2)  & India (70) Georgia (9) Macedonia (5)  \\
Kannada & 0.704 (105) & NA (0) & India (105)  & \\
Sindhi & \textbf{0.190} (96) & 0.062 (9) & India (58) Pakistan (37) Unitedstates (1)  & Macedonia (4) Georgia (2) Indonesia (2)  \\
Italian & \textbf{0.857} (72) & 0.775 (32) & Italy (37) Unitedstates (19) Romania (6)  & India (25) Ireland (2) Spain (2)  \\
Arabic & \textbf{0.732} (58) * & 0.607 (44) & Egypt (19) Jordan (16) Morocco (8)  & Unitedstates (19) India (11) Canada (3)  \\
Telugu & \textbf{0.796} (100) & 0.500 (1) & India (96) Unitedstates (3) Uae (1)  & Saudiarabia (1)  \\
Turkish & 0.749 (69) & \textbf{0.779} (23) & Turkey (37) Unitedstates (16) Macedonia (5)  & India (16) Pakistan (4) Tunisia (1) \\
Breton & 0.167 (3) & \textbf{0.178} (89) & Unitedstates (3)  & India (83) Macedonia (2) China (1) \\
Russian & \textbf{0.148} (64) & 0.120 (24) & Unitedstates (33) Moldova (7) Russia (6)  & India (12) Macedonia (4) Uk (3)  \\
Pashto & \textbf{0.495} (20) * & 0.051 (68) & Pakistan (20)  & India (59) Georgia (2) Macedonia (2)  \\
Irish & \textbf{0.740} (51) & 0.717 (36) & Unitedstates (36) Ireland (13) Uk (2)  & India (28) Romania (3) Pakistan (2)  \\
Tagalog & \textbf{0.849} (76) & 0.825 (9) & Philippines (60) Unitedstates (14) Canada (1)  & Taiwan (2) India (2) Bangladesh (1)  \\
Marathi & 0.654 (82) & \textbf{0.833} (3) & India (80) Mauritius (1) Unitedstates (1)  & Uae (1) Oman (1) Ukraine (1) \\

\hline\hline
\end{tabular}
\normalsize
\end{center}
\caption{Translation quality when partitioning the translations into two groups, one contains translations submitted by Turkers whose location is within regions that plausibly speak the foreign language, and the other contains translations from Turkers outside those regions.} \label{region-summary}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Language skills and location}

We measured the average quality of workers who were in countries that plausibly speak a language, versus workers from countries that did not have large speaker populations of that language.  We used the Ethnologue \cite{ethnologue} to compile the list of countries where each language is spoken.  Table \ref{region-summary} compares the average translation quality of assignments completed within the region of each language, and compares it to the quality of assignments completed outside that region. 

Our workers reported speaking 97 languages natively. US workers alone reported 59 native languages. Overall, 4,465 workers were located in a region likely to speak the language from which they were translating, and 3,170 workers were located in countries considered out of region (meaning almost half of our 5,277 Turkers completed HITs in multiple languages). 

Table \ref{region-summary} shows the differences in translation quality when computed using in-region versus out-of-region Turkers, for the languages with the greatest number of workers.  Within region workers typically produced higher quality translations. 
Given the number of Indian workers on Mechanical Turk, it is unsurprising that they represent majority of out-of-region workers.  For the languages that had more than 80 out of region workers (Malay, Amharic, Afrikaans, Icelandic, and Breton), Indian workers represented 90\% of the out of region workers for all of them aside from Amharic.


%When averaged across all languages, there is no significant quality difference between Turkers in region and out of region (0.504 and 0.496, respectively). However, within each language, the effect of location on quality varies greatly (Table \ref{region-summary}). 

% CCB - commented out this figure and paragraph.  I'm not sure that this figure adds any new information
%
%%%%%%%%%%%%%%%% ASSIGNMENT SCATTER %%%%%%%%%%%
%\begin{figure*}[h]
%\centering
%\includegraphics[width=6in]{final-figures/assign-turk-scatter}
%\caption{Number of assignments and number of Turkers for each language. Each dot represents a language. Figure includes only data from Turkers who reported only one native language consistantly across assignments, representing approx. 3900 Turkers across 139,000 assignments.}
%\label{ass-scatter}
%\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%Figure \ref{ass-scatter} shows the volume of HITs completed and the number of workers participating for each language. While the bulk (19\%) of bilingual Turkers report English as their native language (see Figure \ref{lang-pie}), Mechanical Turk's capacity to support the Indian languages is apparent. Hindi, Tamil, and Malayalam are especially well represented in terms of number of active translators. 



%%%%%%%%%%%%%%% HITLANG QUALITY BAR %%%%%%%%%%%
\begin{figure}[h]
\begin{center}
\includegraphics[width=1\linewidth]{../version1-figures/google_bar.png}
\caption{Average proportion of Turker translations matching GoogleTranslate, per assignment. Overall average overlap was 0.37.}
\label{googlematch} 
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%% QUALITY / NUM ASSIGN SCATTER %%%%%%%%%%%
%\begin{figure}
%\begin{center}
%\includegraphics[width=0.78\linewidth]{../final-figures/quality-scatter_joined-1column}
%\end{center}

%\caption{Estimated quality of translations by country. Each circle is a country, sized proportional to the number of active Turkers from that country. }
%\label{quality-scatter}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Figure \ref{quality-scatter} shows the number of assignments completed per country and per-country quality estimates for 4 scenerios: (a) overall, (b) for assignments completed in region, (c) for assignments completed out of region, and (d) for out of region assignments which contained low overlap with GoogleTranslate. 

\paragraph{Use of machine translation}


\begin{figure}
\centering
\begin{subfigure}[b]{1\linewidth}
\includegraphics[width=\textwidth]{../version1-figures/turker-googmatch-distribution.png}
\caption{Individual turker overlap with google translate. We believe the inflection point at ~60\% overlap with Google suggests a reasonable cutoff by which to filter workers. Workers with \textgreater 60\% overlap can be reasonably considered to be cheating and can be removed.}                
\label{dist}
\end{subfigure}
\begin{subfigure}[b]{1\linewidth}
\includegraphics[width=\textwidth]{../version1-figures/google-cdf-googlangs.png}
\caption{Cumulative distribution of overlap with Google translate for workers and translations. We see that eliminating all workers with \textgreater 60\% overlap with google translate still preserves \textgreater 80\% of workers and \textgreater 70\% of translations.}
\label{cdf}
\end{subfigure}
\caption{}\label{cheaters}
\end{figure}

Given then prevalence of out of region workers, we investigated what proportion of the translations submitted by workers were simply replicating translations from a machine translation system.  Although we followed best practices to deter copying-and-pasting into online MT systems  by  rendering words and sentences as images \cite{zaidan-callisonburch:2011:ACL-HLT2011a}, this strategy does not prevent workers from typing the words into an MT system, if they are able to type in the language's script. 

We used GoogleTranslate to translate all 10,000 words for 50 foreign languages that GoogleTranslate covers. We measured the percent of translations received that exactly matched the translation returned from GoogleTranslate. Results are show in Figure \ref{googlematch}. 
Assignments averaged around 42\% overlap with GoogleTranslate.  Languages with non-Latin scripts (such as the Arabic script, Chinese characters or the Telugu alphabet) have lower overlap with Google,  suggesting that it is easier for Turkers to cheat for languages written in the Latin alphabet. 

\paragraph{Validation of controls} 
Higher overlap with GoogleTranslate corresponds slightly to higher quality scores (Pearson $\rho$ = 0.22), suggesting that some control translations have already been indexed by Google. We therefore investigated the validity of our use of Wikipedia inter-language links as gold standard translations.

As an extrinsic measure of translation quality, we measure the proportion of $<${\it word, translation}$>$ pairs received from Mechanical Turk which appear in external dictionaries.  We had external bilingual dictionaries for 24 of the 119 languages examined in this study.\footnote{The bilingual dictionaries were between English and az, bg, bn, bs, cy, es, fa, hi, id, lv, ms, ne, pl, ro, ru, sk, so, sq, sr, ta, tr, uk, ur, and uz. }  We used each of these bilingual dictionaries to compute language-level quality scores, using the dictionary's translation as a gold standard translation for any of the 10,000 foreign words in our study.  In some cases, the overlap of words was low, since dictionaries list root forms of words and we translated inflected forms.  

We ranked the 24 languages based on their translation quality scores according to the external dictionaries, and calculated correlation when compared to the ranking produced by our gold standard control.  The Pearson correlation was $\rho=0.53$.  We computed a correlation coefficient of $\rho=0.37$ for ranking of the translation quality of individual Turkers using the two sources of reference translations.    

Given the strong positive correlation, we are satisfied that the trends reported in this paper hold when using either our gold standard translations or external bilingual dictionaries. 

\section{Application to SMT}
With the dictionaries in hand, we moved on to translate the entire
Wikipedia documents.  Each human intelligence task (HIT) posted on
MTurk contained ten sequential source-language sentences from a
document, and asked the worker to enter a free-form translation for
each.  We collected four translations from different translators for
each source sentence.  To discourage cheating through
cutting-and-pasting into automatic translation systems, sentences were
presented as images.  Workers were paid \$0.70 per HIT.  We then
manually determined whether to accept or reject a worker's HITs based
on a review of each worker's submissions, which included a comparison
of the translations to a monotonic gloss (produced with the
dictionary), the percentage of empty translations, the amount of time
the worker took to complete the HIT, geographic
location (self-reported and geolocated by way of the worker's
IP address), and by comparing different translations of the same
source segments against one another.

We obtained translations of the
source-language documents in a relatively short amount of time. Malayalam provided the highest throughput,
generating half a million words in just under a week.  For comparison,
the Europarl corpus \cite{koehn2005europarl} has about 50 million
words of English for each of the Spanish and French parallel corpora.

The tradeoff for low-cost translations is increased variance in
translation quality when compared to the more consistently-good
professional translations.  Figure~\ref{figure:variance} contains some
hand-picked examples of the sorts of translations we obtained.  Later,
in the Experiments section (\S\ref{section:experiments}), we will
investigate the effects this variance in translation quality has on
the quality of the models that can be constructed.  For now, the
variance motivated the collection of an additional dataset, described
in the next section.

A prevailing issue with translations collected on MTurk is the
prevalence of low-quality translations.  Quality suffers for a variety
of reasons: Turkers lack formal training, often translate into a
nonnative tongue, may give insufficient attention to the task, and
likely desire to maximize their throughput (and thus their wage).
Unlike \newcite{zaidan2011crowdsourcing}, who embed controls
containing source language sentences with known professional
translations, we had no professionally translated data.  Therefore, we
could not measure the BLEU score of the Turkers.

Motivated by desire to have some measure of the relative quality and variance of the translations,
we designed another task in which we presented an independent set of Turkers with an original
sentence and its four translations, and asked them to vote on which was best.\footnote{We did not
  collect votes for Malayalam.}  Five independent workers voted on the translations of each source
sentence.  Tallying the resulting votes, we found that roughly 65\% of the sentences had five votes
cast on just one or two of the translations, and about 95\% of the sentences had all the votes cast
on one, two, or three sentences.  This suggests both (1) that there was a difference in the quality
of the translations, and (2) the voters were able to discern these differences, and took their task
seriously enough to report them.

\begin{table}[t]
\centering
\begin{tabular}{l|rr}
  language  & SAMT &  Google \\
  \hline\hline
  Bengali    & 13.53  &  20.01 \\
  Hindi      & 17.29  & 25.21 \\  
  Malayalam    & 14.28  & - \\      
  Tamil      &  9.85  &  13.51 \\  
  Telugu     & 12.61  & 16.03  \\  
  Urdu        & 20.99  & 23.09 \\   
\end{tabular}
\caption{BLEU scores translating into English (four references).  BLEU
scores are the mean of three MERT runs.}
\label{table:bleu}
\end{table}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{../version1-figures/tamil.png}
  \begin{tabular}{l}
    \hline
    In March 15,2007 Wiki got a place in Oxford English dictionary. \\
    On March 15, 2007 wiki was included in the Oxford English
    dictionary. (5) \\
    ON MARCH 15, 2007, WIKI FOUND A PLACE IN THE OXFORD ENGLISH
    DICTIONARY \\
    March 15, 2007 oxford english index of wiki's place. \\ \\

  \end{tabular}
  \caption{An example of the variance in translation quality for the
    human translations of a Tamil sentence; the formatting of the
    translations has been preserved exactly.  The parenthesized number
    indicates the number of votes received in the voting task
    (\S\ref{section:votes}).}
  \label{figure:variance}
\end{figure*}


\section{Discussion}
The Mechanical Turk gives research access to a diverse set of bilingual workers, making it a promising resource for researchers and developers of multilingual systems. 
Although unfiltered data can contain large amounts of noise, 
a variety of techniques can be incorporated into crowdsourcing pipelines to ensure high quality data.  Based on our study, we suggest restricting workers to countries that plausibly speak the foreign language of interest, and embedding gold standard controls where possible, rather than relying solely on self-reported language skills.

Although our study targeted bilingual workers on Mechanical Turk, and neglected monolingual workers, we believe our results reliably represent the current speaker populations, since the vast majority of the work available on the crowdsourced platform is currently English-only.  We therefore assume the number of non-English speakers is small.  In the future, it may be desirable to recruit monolingual foreign workers.  In such cases, we recommend other tests to validate their language abilities in place of our translation test.  These could include performing narrative cloze, or listening to audio files containing speech in different language and identifying their language. 


\section{Data release}

We plan to release all data and code used in this study upon publication of this paper.  Our data release will include the raw data, along with bilingual dictionaries that are filtered to be high quality. It will include 308,079 translation assignments from 5,277 Turkers and 20,955 synonym assignments from 1,006 Turkers, along with meta information like geolocation and time submitted, plus external dictionaries used for validation. The dictionaries will contain 850,907 total translated words in 101 languages, along with code to filter the dictionaries based on different criteria. 
 

\bibliographystyle{acl2012}
\bibliography{mturk}

\end{document}
